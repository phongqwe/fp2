\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[final]{neurips_2023}
\usepackage{amssymb}
\usepackage{float}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\title{Mitigating dataset atifact using ....}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Pham The Phong \\
  ptp392\\
   \\
}


\begin{document}


\maketitle


\begin{abstract}


 Models trained for natural language inference can develop spurious correlations between hypotheses and premises.
 These are called dataset artifacts.
  This report details the examination and mitigation of a limited set of dataset artifacts in a natural-language-inference model by retraining and fine-tuning the model on synthetic debiased datasets.
 We use the ELECTRA-small model, train it on the Stanford Natural Language Inference (SNLI) Corpus, and evaluate it using the Heuristic Analysis for NLI Systems (HANS) dataset
 Initially, we train the model on SNLI and establish baseline statistics by testing the baseline model against HANS.
  We then retrain the model from scratch and simultaneously fine-tune the trained baseline model on three synthetic datasets based on SNLI, resulting in six separate models.
  Finally, we perform analysis on the new models, and compare their statistics with the baseline to see if there is any improvement.
  All six models show significantly better performance on HANS, indicating reduced biases compared to the baseline model.
  However, only one outperforms the baseline model on the SNLI test set, while the rest perform worse.

\end{abstract}


\section{Introduction}
Models for natural language inference may contain dataset artifact.
They may assume a relationship (entailment, contrast, or neutral) between a hypothesis and premise base on unrelated factors such as the amount of overlapping words.
In this report, we use the Heuristic Analysis for NLI Systems (HANS) dataset on model ELECTRA-small trained on the Stanford Natural Language Inference (SNLI) Corpus to check for three type of baises (\hyperref[ref1]{McCoy et al.,2019}), as in \hyperref[table1]{Table 1}. Then we will try to de-bias the model by training and finetuning it on existing debiased dataset generated base on SNLI (\hyperref[ref2]{Yuxiang Wu et al.,2022}).



\begin{table}[h]
    \label{table1}
    \caption{Three types of biases}
\centering
\begin{tabular}{p{3cm} p{5cm} p{5cm}}
\hline
\textbf{Bias}    & \textbf{Definition}    & \textbf{Example}    \\ \hline
Lexical overlap  & A premise entails all hypotheses
constructed from words in the premise  & "\textbf{the dog is running on the road}" wrongly entails "\textbf{the dog is} not \textbf{running on the road}"  \\
Subsequence  & A premise entails all of its contiguous subsequences.  & "I see don't see \textbf{the squirrels running}" wrongly entails "\textbf{the squirrls running}" \\
Constituent  & Assume that a premise entails all complete subtrees in its parse tree. & "if \textbf{the dog is hungry}, it eats" wrongly entails "\textbf{the dog is hungry}"  \\ \hline
\end{tabular}

\end{table}

\subsection{Approach}
Our approach is as follows:
\begin{itemize}
  \item First we train ELECTRA-small on SNLI. The trained model is called \textbf{baseline model}.
  \item Then we run the baseline model against the test set of SNLI and HANS to harvest baseline statistic which include:
    \begin{itemize}
      \item overall accuracy on test set of SNLI
      \item overall accuracy on test set of HANS
      \item breakdown accuracy on test set of SNLI by type of bias and by label
      \item breakdown accuracy on test set of HANS by type of bias and by label
    \end{itemize}
  \item Then we train 3 new models and finetune the baseline model using the following 3 synthetic dataset created by \hyperref[ref2]{Yuxiang Wu et al.,2022}, this will result in 6 new models:
    \begin{itemize}
      \item SNLI Par-Z
      \item SNLI Seq-A
      \item SNLI Aug-Z
    \end{itemize}
  \item Finally, we run the 6 new models on SNLI and HANS to harvest new statistics and compare them with the baseline statistic to form our conclusion.
\end{itemize}

\subsection{HANS dataset}
The HANS dataset is a dataset of 60000 samples split evenly between a \textbf{train} subset and a \textbf{validation} subset.
Each sample is labeled either \textbf{entailment} (0) or \textbf{not entailment} (1).
In this report, we use both the train and validation subset for our testing purpose.

\subsection{Synthetic datasets}
The synthetic datasets used in this report are generated by combining the original SNLI with a pure synthetic SNLI, and apply a filter to remove samples that contains spurious correlations measured in z-statistic. (\hyperref[ref2]{Yuxiang Wu et al.,2022}).

Let $\mathcal{D}$ be the original SNLI, and $\mathcal{D}_{S}$ be the synthetic SNLI.
\begin{itemize}
  \item \textbf{SNLI Par-Z} ({\it$\mathcal{Z}(\mathcal{D}) \cup \mathcal{Z}(\mathcal{D}_{S})$}) is created by conduct z-filter on $\mathcal{D}$ and $\mathcal{D}_{S}$, and merge them (\hyperref[ref2]{Yuxiang Wu et al.,2022}).
  \item \textbf{SNLI Seq-A} ({\it$\mathcal{Z}(\mathcal{D}_{S}|\mathcal{Z}(\mathcal{D}))$}) is created by conduct z-filter on $\mathcal{D}$, then conduct conditional z-filter on $\mathcal{D}_{S}$ with $\mathcal{Z}(\mathcal{D})$ as seed (\hyperref[ref2]{Yuxiang Wu et al.,2022}).
  \item \textbf{SNLI Aug-Z} ({\it $\mathcal{Z}(\mathcal{D}_{S}|\mathcal{D})$}) is by conduct conditional z-filter on  $\mathcal{D}_{S}$ using $\mathcal{D}$ as seed (\hyperref[ref2]{Yuxiang Wu et al.,2022})
\end{itemize}

\section{Baseline analysis}
\subsection{Analysis on SNLI test set and HANS}
Running the baseline model on SNLI and HANS produce the overall accuracy (\hyperref[table2]{Table 2}).

\begin{table}[h]
    \label{table2}
    \caption{Baseline model overall accuracies}
    \centering
    \begin{tabular}{l l}
    \hline
    \textbf{Dataset}    & \textbf{Accuracy}    \\ \hline
    SNLI test set  & 88.884\%        \\
    HANS  & 50.715\%        \\ \hline
    \end{tabular}

\end{table}


The accuracy on both HANS train set and validation set is 50.715\% which is significantly lower than the accuracy 88.88\% from SNLI test set.
This indicates that the baseline model is susceptible to the three type of bias describe above.

From SNLI test set, we extract three subsets that contain only overlapping (with Jaccard score > 0.5), subsequence, and constituent samples.
The item count of each subset is in \hyperref[table3]{Table 3}.


\begin{table}[H]
    \caption{Item count of subsets by bias type and gold label in SNLI test set}
\label{table3}
\centering
\begin{tabular}{l l l l}
\hline
\textbf{}            & \textbf{Entail} & \textbf{Contrast} & \textbf{Neutral} \\ \hline
\textbf{Overlap}     & 345    & 175      & 100    \\
\textbf{Constituent} & 28     & 0        & 0      \\
\textbf{Subsequence} & 0      & 0        & 0      \\ \hline
\end{tabular}

\end{table}

This breakdown shows that the SNLI test set is highly skewed and contains a small number of samples that belong to one of the 3 biases.
It does not contain any subsequence sample, and all of the constituent samples are labeled entailment.
Overlap subset is larger than the others, but is skewed toward entailment.
Therefore, SNLI test set can be only used to measure the overall performance and performance on samples that do not belong to one of the three biases.
However we still run the baseline model each of these subset to get the accuracy on these subsets, in \hyperref[table4]{Table 4}.

\begin{table}[h]
    \caption{Accuracy (\%) per subset (by bias type and gold label) of SNLI test set}
    \label{table4}
\centering
\begin{tabular}{l l l l}
\hline
\textbf{} & \textbf{Entail} & \textbf{Contrast} & \textbf{Neutral} \\ \hline
\textbf{Overlap}     & 92.17         & 17.71           & 3.00           \\
\textbf{Constituent} & 85.71         & N/A               & N/A              \\
\textbf{Subsequence} & N/A             & N/A               & N/A              \\ \hline
\end{tabular}


\end{table}


We perform similar breakdown on HANS for all three type of biases and 2 gold label(entailment and non-entailment) (\hyperref[table5]{Table 5} and \hyperref[table6]{Table 6}).
The baseline model correctly classifies 96-100\% of entail sample and at the same time it incorrectly classify nearly all the non-entail sample.
This means the model is equally susceptible to all three type of bias.

\begin{table}[H]
    \caption{Item count of subsets by bias type and gold label in HANS}
\label{table5}
\centering
\begin{tabular}{l l l l}
\hline
\textbf{}            & \textbf{Entail} & \textbf{Non-entail} \\ \hline
\textbf{Overlap}     & 10000    & 10000         \\
\textbf{Constituent} & 10000     & 10000              \\
\textbf{Subsequence} & 10000      & 10000             \\ \hline
\end{tabular}

\end{table}
\begin{table}[h]
    \label{table6}
    \caption{classify result (\%) per subset (by bias type and gold label) in HANS}
\centering
\begin{tabular}{l l l}
\hline
\textbf{} & \textbf{Entailment} & \textbf{Non-entailment} \\ \hline
\textbf{Overlap} & 96.640 & 6.990 \\
\textbf{Subsequence} & 99.560 & 0.660 \\
\textbf{Constituent} & 100.000 & 0.440 \\ \hline
\end{tabular}
\end{table}

\subsection{Analysis on SNLI train set}
We make another breakdown on SNLI train set in \hyperref[table6]{Table 6} to see if train data set plays any role in the bias of the baseline model.

\begin{table}[h]
    \label{table7}
    \caption{Breakdown of item count by bias type and gold label}
\centering
\begin{tabular}{l l l l}
\hline
\textbf{} & \textbf{Entailment} & \textbf{Contrast} & \textbf{Neutral} \\ \hline
\textbf{Overlap}     & 18938 & 11801 & 7410 \\
\textbf{Subsequence} & 274   & 26    & 20   \\
\textbf{Constituent} & 1054  & 36    & 13   \\ \hline
\end{tabular}

\end{table}

\hyperref[table7]{Table 7} shows that compare to \textbf{contrast} and \textbf{neutral}, \textbf{entailment} has:
\begin{itemize}
    \item The number of overlap is 1.6 and 2.5 times higher respectively.
    \item The number of subsequence is 10 times higher.
    \item The number of constituent is 29 and 81 times higher respectively.
\end{itemize}
 This means given a sample that is either overlap, subsequence, or constituent, it is more likely to be classified as \textbf{entailment} than \textbf{contrast} or \textbf{neutral}.
In other words, the train dataset is skewed toward \textbf{entailment} for samples that fall into one of the three biases.

\section{Mitigating}

To mitigate the artifact in the baseline model, we train the model on three debiased dataset generated from SNLI (SNLI Par-Z, SNLI Seq-Z, and SNLI Aug-Z), at the same time, we finetune the baseline model on said datasets.
This results in 6 new models.
For each, we compute the same statistic using the new models on the HANS and SNLI test set.

\subsection{Overall accuracy}

\begin{table}[H]
\label{table8}
\caption{Model accuracy on SNLI test set}
\centering
\begin{tabular}{l l}
\hline
\textbf{Model}                   & \textbf{Accuracy (\%)} \\ \hline
\textbf{Baseline}                & 88.884                 \\
\textbf{Trained on SNLI Par-Z}   & 85.596                 \\
\textbf{Finetuned on SNLI Par-Z} & 88.101                 \\
\textbf{Trained on SNLI Seq-Z}   & 85.719                 \\
\textbf{Finetuned on SNLI Seq-Z} & 88.314                 \\
\textbf{Trained on SNLI Z-Aug}   & 88.569                 \\
\textbf{Finetuned on SNLI Z-Aug} & \textbf{89.057}                 \\ \hline
\end{tabular}
\end{table}

From \hyperref[table8]{Table 8}, of the 6 new model, only the model finetuned on SNLI Z-Aug perform slightly better than the baseline model.
The finetuned models on SNLI Par-Z and SNLI Seq-Z are slightly worse than the baseline, while the two trained from scratch on those datasets perform significantly worse with about 3\% less than the baseline.

\begin{table}[H]
    \label{table9}
    \caption{Model accuracy on HANS}
\centering
\begin{tabular}{l l}
\hline
\textbf{Model}                   & \textbf{Accuracy (\%)} \\ \hline
\textbf{Baseline}                & 50.715                 \\
\textbf{Trained on SNLI Par-Z}   & 58.703                 \\
\textbf{Finetuned on SNLI Par-Z} & 57.947                 \\
\textbf{Trained on SNLI Seq-Z}   & 56.782                 \\
\textbf{Finetuned on SNLI Seq-Z} & 58.433                 \\
\textbf{Trained on SNLI Z-Aug}   & 59.917                 \\
\textbf{Finetuned on SNLI Z-Aug} & 55.817                 \\ \hline
\end{tabular}

\end{table}
From \hyperref[table9]{Table 9}, all the new models perform better than the base model by 5-9\% on HANS
\subsection{Breakdown accuracy and individual improvements}

A further breaking down HANS result by bias type and label (\hyperref[table10]{Table 10}) show that all 6 new models perform better on all non-entailment subset and slightly worse on entailment.
The large improvement is on classifying non-entailment for overlapping sample.
The smallest improvement is 31.49\% while the largest is 52.97\%
For other subset, the improvement is significantly less ranging from 19\% to as low as around 2\%.
This means the new models are much better at \textbf{not} mistaking overlapping samples as entailment, but not that much (still better, but not much) for other type of biases.

\section{Discussion}
\subsection{Deviation between debias dataset}
There are signification differences between the result of model trained on debiased dataset and model finetunes on the same dataset.
There are so significant differences between models train on different dataset, or between models finetuned on different dataset.

For the same data set, a trained model perform significantly better than a finetuned model in term of 


%\begin{table}[]
%     \label{table10}
%    \caption{Break down result on HANS by bias type and label}
%
%\begin{tabular}{lllrrl}
%\textbf{model}                   & \textbf{bias type}   & \textbf{label}      & \multicolumn{1}{l}{\textbf{accuracy}} & \multicolumn{1}{l}{\textbf{Compare to baseline}} & \textbf{better or not} \\ \hline
%baseline                & overlap     & non-entail & 6.990\%                      & 0.000\%                                 & same          \\
%                        &             & entail     & 96.640\%                     & 0.000\%                                 & same          \\
%                        & subsequence & non-entail & 0.440\%                      & 0.000\%                                 & same          \\
%                        &             & entail     & 100.000\%                    & 0.000\%                                 & same          \\
%                        & constituent & non-entail & 0.660\%                      & 0.000\%                                 & same          \\
%                        &             & entail     & 99.560\%                     & 0.000\%                                 & same          \\
%trained on SNLI Par-Z   & overlap     & non-entail & 48.280\%                     & 41.290\%                                & better        \\
%                        &             & entail     & 85.980\%                     & -10.660\%                               & worse         \\
%                        & subsequence & non-entail & 13.510\%                     & 13.070\%                                & better        \\
%                        &             & entail     & 97.640\%                     & -2.360\%                                & worse         \\
%                        & constituent & non-entail & 19.720\%                     & 19.060\%                                & better        \\
%                        &             & entail     & 87.090\%                     & -12.470\%                               & worse         \\
%finetuned on SNLI Par-Z & overlap     & non-entail & 53.590\%                     & 46.600\%                                & better        \\
%                        &             & entail     & 89.160\%                     & -7.480\%                                & worse         \\
%                        & subsequence & non-entail & 4.870\%                      & 4.430\%                                 & better        \\
%                        &             & entail     & 98.750\%                     & -1.250\%                                & worse         \\
%                        & constituent & non-entail & 4.920\%                      & 4.260\%                                 & better        \\
%                        &             & entail     & 96.390\%                     & -3.170\%                                & worse         \\
%trained on SNLI Seq-Z   & overlap     & non-entail & 38.480\%                     & 31.490\%                                & better        \\
%                        &             & entail     & 94.240\%                     & -2.400\%                                & worse         \\
%                        & subsequence & non-entail & 5.330\%                      & 4.890\%                                 & better        \\
%                        &             & entail     & 98.650\%                     & -1.350\%                                & worse         \\
%                        & constituent & non-entail & 9.060\%                      & 8.400\%                                 & better        \\
%                        &             & entail     & 94.930\%                     & -4.630\%                                & worse         \\
%finetuned on SNLI Seq-Z & overlap     & non-entail & 56.930\%                     & 49.940\%                                & better        \\
%                        &             & entail     & 89.720\%                     & -6.920\%                                & worse         \\
%                        & subsequence & non-entail & 4.200\%                      & 3.760\%                                 & better        \\
%                        &             & entail     & 99.460\%                     & -0.540\%                                & worse         \\
%                        & constituent & non-entail & 3.430\%                      & 2.770\%                                 & better        \\
%                        &             & entail     & 96.860\%                     & -2.700\%                                & worse         \\
%trained on SNLI Z-Aug   & overlap     & non-entail & 59.960\%                     & 52.970\%                                & better        \\
%                        &             & entail     & 90.180\%                     & -6.460\%                                & worse         \\
%                        & subsequence & non-entail & 9.790\%                      & 9.350\%                                 & better        \\
%                        &             & entail     & 98.670\%                     & -1.330\%                                & worse         \\
%                        & constituent & non-entail & 6.170\%                      & 5.510\%                                 & better        \\
%                        &             & entail     & 94.730\%                     & -4.830\%                                & worse         \\
%finetuned on SNLI Z-Aug & overlap     & non-entail & 38.830\%                     & 31.840\%                                & better        \\
%                        &             & entail     & 92.800\%                     & -3.840\%                                & worse         \\
%                        & subsequence & non-entail & 3.190\%                      & 2.750\%                                 & better        \\
%                        &             & entail     & 99.680\%                     & -0.320\%                                & worse         \\
%                        & constituent & non-entail & 3.320\%                      & 2.660\%                                 & better        \\
%                        &             & entail     & 97.080\%                     & -2.480\%                                & worse        \\ \hline
%\end{tabular}
%\end{table}


\begin{table}[]
    \label{table10}
    \caption{Break down result on HANS by bias type and label}
    \begin{tabular}{lllrrl}
        \textbf{Model}                   & \textbf{Bias type}   & \textbf{Label}      & \multicolumn{1}{l}{\textbf{Accuracy}} & \multicolumn{1}{l}{\textbf{Compare to baseline}} & \textbf{Better or not} \\ \hline
        baseline                         & overlap             & non-entail          & 6.990\%                                & 0.000\%                                         & same          \\
                                          &                     & entail              & 96.640\%                               & 0.000\%                                         & same          \\
                                          & subsequence         & non-entail          & 0.440\%                                & 0.000\%                                         & same          \\
                                          &                     & entail              & 100.000\%                              & 0.000\%                                         & same          \\
                                          & constituent         & non-entail          & 0.660\%                                & 0.000\%                                         & same          \\
                                          &                     & entail              & 99.560\%                               & 0.000\%                                         & same          \\
        trained on SNLI Par-Z            & overlap             & non-entail          & 48.280\%                               & \textbf{41.290\%}                               & better        \\
                                          &                     & entail              & 85.980\%                               & -10.660\%                                       & worse         \\
                                          & subsequence         & non-entail          & 13.510\%                               & \textbf{13.070\%}                               & better        \\
                                          &                     & entail              & 97.640\%                               & -2.360\%                                        & worse         \\
                                          & constituent         & non-entail          & 19.720\%                               & \textbf{19.060\%}                               & better        \\
                                          &                     & entail              & 87.090\%                               & -12.470\%                                       & worse         \\
        finetuned on SNLI Par-Z          & overlap             & non-entail          & 53.590\%                               & \textbf{46.600\%}                               & better        \\
                                          &                     & entail              & 89.160\%                               & -7.480\%                                        & worse         \\
                                          & subsequence         & non-entail          & 4.870\%                                & \textbf{4.430\%}                                & better        \\
                                          &                     & entail              & 98.750\%                               & -1.250\%                                        & worse         \\
                                          & constituent         & non-entail          & 4.920\%                                & \textbf{4.260\%}                                & better        \\
                                          &                     & entail              & 96.390\%                               & -3.170\%                                        & worse         \\
        trained on SNLI Seq-Z            & overlap             & non-entail          & 38.480\%                               & \textbf{31.490\%}                               & better        \\
                                          &                     & entail              & 94.240\%                               & -2.400\%                                        & worse         \\
                                          & subsequence         & non-entail          & 5.330\%                                & \textbf{4.890\%}                                & better        \\
                                          &                     & entail              & 98.650\%                               & -1.350\%                                        & worse         \\
                                          & constituent         & non-entail          & 9.060\%                                & \textbf{8.400\%}                                & better        \\
                                          &                     & entail              & 94.930\%                               & -4.630\%                                        & worse         \\
        finetuned on SNLI Seq-Z          & overlap             & non-entail          & 56.930\%                               & \textbf{49.940\%}                               & better        \\
                                          &                     & entail              & 89.720\%                               & -6.920\%                                        & worse         \\
                                          & subsequence         & non-entail          & 4.200\%                                & \textbf{3.760\%}                                & better        \\
                                          &                     & entail              & 99.460\%                               & -0.540\%                                        & worse         \\
                                          & constituent         & non-entail          & 3.430\%                                & \textbf{2.770\%}                                & better        \\
                                          &                     & entail              & 96.860\%                               & -2.700\%                                        & worse         \\
        trained on SNLI Z-Aug            & overlap             & non-entail          & 59.960\%                               & \textbf{52.970\%}                               & better        \\
                                          &                     & entail              & 90.180\%                               & -6.460\%                                        & worse         \\
                                          & subsequence         & non-entail          & 9.790\%                                & \textbf{9.350\%}                                & better        \\
                                          &                     & entail              & 98.670\%                               & -1.330\%                                        & worse         \\
                                          & constituent         & non-entail          & 6.170\%                                & \textbf{5.510\%}                                & better        \\
                                          &                     & entail              & 94.730\%                               & -4.830\%                                        & worse         \\
        finetuned on SNLI Z-Aug          & overlap             & non-entail          & 38.830\%                               & \textbf{31.840\%}                               & better        \\
                                          &                     & entail              & 92.800\%                               & -3.840\%                                        & worse         \\
                                          & subsequence         & non-entail          & 3.190\%                                & \textbf{2.750\%}                                & better        \\
                                          &                     & entail              & 99.680\%                               & -0.320\%                                        & worse         \\
                                          & constituent         & non-entail          & 3.320\%                                & \textbf{2.660\%}                                & better        \\
                                          &                     & entail              & 97.080\%                               & -2.480\%                                        & worse        \\ \hline
    \end{tabular}
\end{table}


\section{Conclusion}


\section*{References}

{
\small

\label{ref1}
[1] R. Thomas McCoy, Ellie Pavlick \ \& Tal Linzen (2019) Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference. In {\it Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 3428â€“3448, Stroudsburg, PA, USA, 2019. Association for Computational Linguistics.

\label{ref2}
[2] Yuxiang Wu, Matt Gardner, Pontus Stenetorp \ \& Pradeep Dasigi (2022) Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets. In {\it Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics}, ACL 2022.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}